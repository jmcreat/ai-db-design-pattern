#!/usr/bin/env python3
"""
SQL ìŠ¤í‚¤ë§ˆì—ì„œ Mermaid ERD ìë™ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
í´ë”ë³„ë¡œ schema.sqlì„ ì½ì–´ì„œ .mmd íŒŒì¼ê³¼ ERD.mdë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""

import re
import sys
from pathlib import Path
import argparse

def parse_sql_schema(sql_file):
    """SQL íŒŒì¼ì—ì„œ í…Œì´ë¸” êµ¬ì¡° íŒŒì‹± (MySQL ë¬¸ë²• ì§€ì›)"""
    tables = {}
    
    with open(sql_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ì£¼ì„ ì œê±° (-- ìŠ¤íƒ€ì¼)
    content = re.sub(r'--[^\n]*', '', content)
    
    # CREATE TABLE íŒ¨í„´ ì°¾ê¸° (MySQL ë¬¸ë²• ì •í™•íˆ ì²˜ë¦¬)
    # íŒ¨í„´: CREATE TABLE table_name ( ... ) [COMMENT='...'];
    table_pattern = r'CREATE TABLE\s+(\w+)\s*\(((?:[^()]|\((?:[^()]|\([^()]*\))*\))*)\)\s*(?:COMMENT\s*=?\s*[\'"][^\'"]*[\'"])?\s*;'
    matches = re.findall(table_pattern, content, re.DOTALL | re.IGNORECASE)
    
    for table_name, table_body in matches:
        columns = []
        
        # FK ì»¬ëŸ¼ ì°¾ê¸° (í…Œì´ë¸” ë ˆë²¨ FK)
        fk_columns = set()
        fk_pattern = r'FOREIGN KEY\s*\((\w+)\)\s*REFERENCES'
        fk_matches = re.findall(fk_pattern, table_body, re.IGNORECASE)
        for fk_col in fk_matches:
            fk_columns.add(fk_col.strip())
        
        # ì»¬ëŸ¼ë³„ë¡œ ë¼ì¸ ë¶„ë¦¬
        lines = table_body.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # ë¹ˆ ì¤„ì´ë‚˜ ì£¼ì„ ìŠ¤í‚µ
            if not line or line.startswith('--'):
                continue
            
            # ì œì•½ì¡°ê±´ ë¼ì¸ ìŠ¤í‚µ
            if re.match(r'^\s*(PRIMARY\s+KEY|FOREIGN\s+KEY|UNIQUE|INDEX|KEY|CHECK|CONSTRAINT)', line, re.IGNORECASE):
                continue
            
            # ì»¬ëŸ¼ íŒŒì‹±
            # íŒ¨í„´: column_name data_type [constraints] [COMMENT '...']
            col_match = re.match(r'^(\w+)\s+([A-Z]+(?:\([^)]*\))?)', line, re.IGNORECASE)
            
            if not col_match:
                continue
            
            col_name = col_match.group(1).strip()
            data_type_raw = col_match.group(2).strip().upper()
            
            # ë°ì´í„° íƒ€ì… ì •ê·œí™”
            if 'BIGINT' in data_type_raw or 'INT' in data_type_raw or 'SERIAL' in data_type_raw:
                data_type = 'bigint' if 'BIGINT' in data_type_raw else 'int'
            elif 'VARCHAR' in data_type_raw or 'CHAR' in data_type_raw:
                data_type = 'varchar'
            elif 'DECIMAL' in data_type_raw or 'NUMERIC' in data_type_raw:
                data_type = 'decimal'
            elif 'TIMESTAMP' in data_type_raw:
                data_type = 'timestamp'
            elif 'DATETIME' in data_type_raw:
                data_type = 'datetime'
            elif 'DATE' in data_type_raw:
                data_type = 'date'
            elif 'TEXT' in data_type_raw or 'JSON' in data_type_raw:
                data_type = 'text'
            elif 'BOOLEAN' in data_type_raw or 'BOOL' in data_type_raw:
                data_type = 'boolean'
            else:
                data_type = 'varchar'
            
            # ì œì•½ì¡°ê±´ ì¶”ì¶œ
            constraint = ''
            if 'PRIMARY KEY' in line.upper() or 'PK' in line.upper():
                constraint = 'PK'
            elif col_name in fk_columns or 'REFERENCES' in line.upper():
                constraint = 'FK'
            elif 'UNIQUE' in line.upper():
                constraint = 'UK'
            
            # COMMENT ì¶”ì¶œ (ì„ íƒì )
            comment_match = re.search(r"COMMENT\s+'([^']*)'", line, re.IGNORECASE)
            comment = ''
            if comment_match:
                comment = f'"{comment_match.group(1)}"'
            
            # ì»¬ëŸ¼ ë¬¸ìì—´ ìƒì„±
            column_str = f"{data_type} {col_name}"
            if constraint:
                column_str += f" {constraint}"
            if comment:
                column_str += f" {comment}"
            
            columns.append(column_str.strip())
        
        if columns:  # ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
            tables[table_name] = columns
    
    return tables

def parse_relationships(sql_file):
    """SQL íŒŒì¼ì—ì„œ ì™¸ë˜í‚¤ ê´€ê³„ íŒŒì‹± (MySQL ë¬¸ë²• ì§€ì›)"""
    relationships = []
    
    with open(sql_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ì£¼ì„ ì œê±°
    content = re.sub(r'--[^\n]*', '', content)
    
    # CREATE TABLE ë¸”ë¡ë³„ë¡œ ë¶„ë¦¬
    table_pattern = r'CREATE TABLE\s+(\w+)\s*\(((?:[^()]|\((?:[^()]|\([^()]*\))*\))*)\)\s*(?:COMMENT\s*=?\s*[\'"][^\'"]*[\'"])?\s*;'
    table_matches = re.findall(table_pattern, content, re.DOTALL | re.IGNORECASE)
    
    for table_name, table_body in table_matches:
        # FOREIGN KEY (column) REFERENCES ref_table(ref_column)
        fk_pattern = r'FOREIGN KEY\s*\((\w+)\)\s*REFERENCES\s+(\w+)\s*\((\w+)\)'
        fk_matches = re.findall(fk_pattern, table_body, re.IGNORECASE)
        
        for fk_col, ref_table, ref_col in fk_matches:
            relationships.append((table_name, ref_table, fk_col, ref_col))
    
    # ì¤‘ë³µ ì œê±°
    relationships = list(set(relationships))
    
    return relationships

def generate_mermaid_erd(tables, relationships):
    """Mermaid ERD ì½”ë“œ ìƒì„±"""
    mermaid_code = "erDiagram\n"
    
    # í…Œì´ë¸” ì •ì˜
    for table_name, columns in tables.items():
        mermaid_code += f"    {table_name} {{\n"
        for column in columns:
            mermaid_code += f"        {column}\n"
        mermaid_code += "    }\n\n"
    
    # ê´€ê³„ ì •ì˜
    mermaid_code += "    %% Relationships\n"
    for table1, table2, fk_col, ref_col in relationships:
        mermaid_code += f"    {table2} ||--o{{ {table1} : \"{fk_col}\"\n"
    
    return mermaid_code

def update_mermaid_file(mermaid_code, output_file):
    """Mermaid íŒŒì¼ ì—…ë°ì´íŠ¸"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(mermaid_code)

def update_markdown_file(mermaid_code, markdown_file):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì˜ Mermaid ì½”ë“œ ë¸”ë¡ ì—…ë°ì´íŠ¸"""
    with open(markdown_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Mermaid ì½”ë“œ ë¸”ë¡ ì°¾ê¸° ë° êµì²´
    pattern = r'```mermaid\n(.*?)\n```'
    replacement = f'```mermaid\n{mermaid_code}```'
    
    updated_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
    
    with open(markdown_file, 'w', encoding='utf-8') as f:
        f.write(updated_content)

def process_folder(folder_path):
    """íŠ¹ì • í´ë”ì˜ schema.sqlì„ ì²˜ë¦¬í•˜ì—¬ ERD ìƒì„±"""
    folder = Path(folder_path)
    
    if not folder.exists() or not folder.is_dir():
        print(f"âŒ Error: {folder_path} is not a valid directory")
        return False
    
    sql_file = folder / 'schema.sql'
    mermaid_file = folder / 'ERD.mmd'
    markdown_file = folder / 'ERD.md'
    
    if not sql_file.exists():
        print(f"âš ï¸  Skipping {folder.name}: schema.sql not found")
        return False
    
    print(f"\nğŸ“‚ Processing {folder.name}/")
    print(f"   Reading: {sql_file.name}")
    
    try:
        tables = parse_sql_schema(sql_file)
        relationships = parse_relationships(sql_file)
        
        if not tables:
            print(f"   âš ï¸  No tables found in {sql_file.name}")
            return False
        
        print(f"   Found: {len(tables)} tables, {len(relationships)} relationships")
        
        mermaid_code = generate_mermaid_erd(tables, relationships)
        
        # .mmd íŒŒì¼ ìƒì„±
        update_mermaid_file(mermaid_code, mermaid_file)
        print(f"   âœ… Created: {mermaid_file.name}")
        
        # ERD.md íŒŒì¼ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
        if markdown_file.exists():
            update_markdown_file(mermaid_code, markdown_file)
            print(f"   âœ… Updated: {markdown_file.name}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Error processing {folder.name}: {e}")
        return False

def find_domain_folders(base_path='.'):
    """ë„ë©”ì¸ í´ë” ìë™ íƒìƒ‰"""
    base = Path(base_path)
    
    # schema.sqlì„ í¬í•¨í•œ í´ë”ë§Œ ì„ íƒ
    domain_folders = []
    
    for item in base.iterdir():
        if item.is_dir() and not item.name.startswith('.'):
            schema_file = item / 'schema.sql'
            if schema_file.exists():
                domain_folders.append(item)
    
    return sorted(domain_folders)

def main():
    parser = argparse.ArgumentParser(
        description='SQL ìŠ¤í‚¤ë§ˆì—ì„œ Mermaid ERD ìë™ ìƒì„±',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ëª¨ë“  ë„ë©”ì¸ í´ë” ì²˜ë¦¬
  python update_erd.py
  
  # íŠ¹ì • í´ë”ë§Œ ì²˜ë¦¬
  python update_erd.py finance
  python update_erd.py finance ecommerce
        """
    )
    
    parser.add_argument(
        'folders',
        nargs='*',
        help='ì²˜ë¦¬í•  í´ë” (ë¯¸ì§€ì •ì‹œ ëª¨ë“  ë„ë©”ì¸ í´ë” ìë™ ì²˜ë¦¬)'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ”„ ERD Generator - SQL to Mermaid")
    print("=" * 60)
    
    if args.folders:
        # ì§€ì •ëœ í´ë”ë§Œ ì²˜ë¦¬
        folders_to_process = [Path(f) for f in args.folders]
    else:
        # ëª¨ë“  ë„ë©”ì¸ í´ë” ìë™ íƒìƒ‰
        folders_to_process = find_domain_folders()
        if folders_to_process:
            print(f"\nğŸ“ Found {len(folders_to_process)} domain folders:")
            for folder in folders_to_process:
                print(f"   - {folder.name}/")
        else:
            print("\nâš ï¸  No domain folders with schema.sql found")
            return
    
    # ì²˜ë¦¬
    success_count = 0
    for folder in folders_to_process:
        if process_folder(folder):
            success_count += 1
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    if success_count > 0:
        print(f"âœ… Successfully processed {success_count}/{len(folders_to_process)} folders")
    else:
        print("âŒ No folders were processed successfully")
    print("=" * 60)

if __name__ == "__main__":
    main()
